{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2bb8db9990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 6\n",
    "eval_interval = 100\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, mode =\"r\", encoding=\"utf-8\") as f:\n",
    "            self.text = f.read()\n",
    "            self.unique_chars = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.unique_chars)\n",
    "        self.mapping = self.generate_char_mappings(self.unique_chars)\n",
    "    \n",
    "    def generate_char_mappings(self, uq):\n",
    "        char_to_idx = {c : i for i, c in enumerate(uq)}\n",
    "        idx_to_char = {i : c for i, c in enumerate(uq)}\n",
    "        return {'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char}\n",
    "        \n",
    "    def convert_seq_to_indices(self, seq):\n",
    "        return [self.mapping['char_to_idx'][char] for char in seq]\n",
    "\n",
    "    def convert_indices_to_seq(self, seq):\n",
    "        return \"\".join([self.mapping['idx_to_char'][idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 64])\n",
      "torch.Size([32, 32])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Create a custom dataset class\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Grab a chunk of data with block_size + 1 length\n",
    "        chunk = self.data[index:index + self.block_size + 1]\n",
    "        # Input sequence (x) is all but the last character in the chunk\n",
    "        x = chunk[:-1]\n",
    "        # Target sequence (y) is all but the first character in the chunk, shifted by one\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "# Load the necessary data\n",
    "class Dataloader():\n",
    "    def __init__(self, Data):\n",
    "        self.chars = Data.unique_chars\n",
    "        self.vocab_size = Data.vocab_size\n",
    "        self.stoi = Data.mapping['char_to_idx']\n",
    "        self.itos = Data.mapping['idx_to_char']\n",
    "\n",
    "        # create a mapping from characters to integers\n",
    "        self.encode = lambda s: Data.convert_seq_to_indices(s) # encoder: take a string, output a list of integers\n",
    "        self.decode = lambda l: Data.convert_indices_to_seq(l) # decoder: take a list of integers, output a string\n",
    "        self.data = torch.tensor(self.encode(Data.text))    \n",
    "        self.train_split, self.valid_split, self.test_split = self.create_data_split()\n",
    "        \n",
    "\n",
    "    def create_data_split(self, train = 0.8, valid = 0.1, test = 0.1):\n",
    "        # Have to ensure total split sums up to 1\n",
    "        assert train + valid + test == 1, \"Total Split Must Sum Up to 1\"\n",
    "        length_data = len(self.data)\n",
    "\n",
    "        train_n = int(train * length_data)\n",
    "        valid_n = int(valid * length_data)\n",
    "        test_n = int(test * length_data)\n",
    "\n",
    "        # Ensure that each data split has at least one sample\n",
    "        assert train_n > 0, \"Training split has zero elements\"\n",
    "        assert valid_n > 0, \"Validation split has zero elements\"\n",
    "        assert test_n > 0, \"Test split has zero elements\"\n",
    "\n",
    "        # Create splits\n",
    "        train_data = self.data[ : train_n]\n",
    "        valid_data = self.data[train_n : train_n + valid_n]\n",
    "        test_data = self.data[train_n + valid_n :] \n",
    "\n",
    "        # Return splits\n",
    "        return train_data, valid_data, test_data\n",
    "    \n",
    "    def build_loader(self, split):\n",
    "        if split == \"train\":\n",
    "            dataset = TransformerDataset(self.train_split, block_size)\n",
    "        elif split == \"valid\":\n",
    "            dataset = TransformerDataset(self.valid_split, block_size)\n",
    "        else:\n",
    "            dataset = TransformerDataset(self.test_split, block_size)\n",
    "        \n",
    "        shuffle = split == \"train\"\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle= shuffle)\n",
    "\n",
    "        return dataloader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A head of decoder self-attention\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size): \n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(n_embd, self.head_size)\n",
    "        self.key = nn.Linear(n_embd, self.head_size)\n",
    "        self.value = nn.Linear(n_embd, self.head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = True):\n",
    "        k = self.key(x) # B, T, C (head_size)\n",
    "        q = self.query(x) # B, T, C (head_size)\n",
    "        attn = q @ k.transpose(-2, -1) / math.sqrt(self.head_size) # Following the actual implementation for Attention # (B, T, C) * (B, C, T) => (B, T, T)\n",
    "        # Decoder Only Model, meaning we can't see into the future, hence masking is needed prior to softmax operation\n",
    "        if mask == True:\n",
    "            attn = attn.masked_fill(self.tril[:x.shape[-2], :x.shape[-2]] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim = -1) # Apply softmax\n",
    "        attn = self.dropout(attn)\n",
    "        v = self.value(x) # B, T, C\n",
    "        attn = attn @ v # B, T, T @ B, T, C\n",
    "        return attn\n",
    "    \n",
    "\n",
    "# A head of Masked Multi Head Attention\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x, mask = True) for h in self.heads], dim = -1) # the channel dimension will equal to num of embedding\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Feedforward layer\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Decoder-only Block\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MaskedMultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual + pre - norm layer\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitions and key notes:\n",
    "## Self-Attention:\n",
    "#### High Level Overview:\n",
    "Consisting of 3 main keys(no pun intended):\n",
    "- Key: Here it mainly focuses on the position of the words in the sequence\n",
    "- Query: Acts as a Scanner or seeker that focuses on specific feature / aspect of a sequence, in terms of langauge, on a word level, it can be thought of scanning for high level concepts\n",
    "- Value: Actually cntain the actual information and data to be attended to and combined based on the relevance score computed from the keys and queries\n",
    "Essentially this allows the self-attention block to capture long term dependencies and relationships within the input \n",
    "#### Mathematical Intuition:\n",
    "In pytorch, these key, query, value can be thought of as a linear layer, normally set to having the same dimension, normally the dimensions for these layers are the division of the number of embedding over the number of head, . First we do a matrix multiplication of the query and key to find out the relevance score, afterwards it is often scaled down by square root of the head size (calculated by the embedding dimension / number of head). Why should it be scaled down? Because often times the result of the dot product would yield us with values too big / too small and having that as an input to a softmax function would essentially led to having high variance, resulting in vanishing / exploding gradients and the saturation of the softmax function(leading to very big / small attention weights, making it hard to learn meaningful patterns) Afterwards we multiply it with the value\n",
    "Regarfding masking:\n",
    "In the case of a decoder transformer, that means the input sequence at a specific timestep can't look into the future, so we do it by masking and replacing the future values by -infinity, since we are gonna softmax the value, otherwise encoder transformer will look into every input at every timestep\n",
    "\n",
    "## Masked Multi Head Attention:\n",
    "#### High Level Overview:\n",
    "Can be thought of as a module or a list of attention blocks, where each attention block represents a separate \"head\" (Each head has a separate set of values). For each attention block, attention computation is performed independently, meaning you can uncover underlying patterns and whatnot. The attended values for each head are then concatenated across the head size dimension (stacked together), and then a final projection layer is applied to produce the output of the multi-head attention layer\n",
    "\n",
    "## Residual Connection:\n",
    "#### High Level Overview:\n",
    "Can be thought of as the \"skip-connection\" that allows inputs to traverse deep within the network, this ensures that during the backpropagation, the gradients can flow through 2 parts, namely the layer itself and the skip connection\n",
    "\n",
    "## Dropout:\n",
    "#### High Level Overview:\n",
    "Regularization technique used during training of model (they are automatically deactivated during inference / evaluation / testing), what it does is it deactivates some of the neurons(units) in a layer by setting them to zero depending on the hyperparameter specified. The deactivation of random neurons introduce noise in the network, which acts as regularizer and helps prevent the model from overfitting to the data\n",
    "\n",
    "## LayerNormalization:\n",
    "#### High Level Overview:\n",
    "For consistent scale and distribution, improve training process and prevent issues like vanishing / exploding gradients. In layer normalization, you essentially normalize each row separately in the batch (across dim = 1 if it's 2d). So for each row, the same mean / standard deviation is used to normalize the row feature/activation value\n",
    "\n",
    "## Feed Forward Layer:\n",
    "#### High Level Overview:\n",
    "Essentially just a dense linear layer stacked on top of each other, with activation layers like relu,tanh in betweem to introduce non-linearities, finished off with a dropout layer.\n",
    "\n",
    "## Decoder Block:\n",
    "#### High Level Overview:\n",
    "You put everything together based on the concepts outlined above:\n",
    "input -> layer norm -> multi-head attention -> residual -> layernorm -> feedforward -> residual -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Only Transformer\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(data.vocab_size, n_embd) \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #Positional Embedding, depends on block size\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, data.vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of ints\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C), generate integers from 0 to T-1\n",
    "        x = tok_emb + pos_emb # (B,T,C) Adding token embedding with position embedding\n",
    "        x = self.blocks(x) # (B,T,C), Layers of masked multi head attention # C = n_embeds\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens, preventing segfault\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # predictions, do forward\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 23, 12,  ..., 44, 57, 44],\n",
      "        [57,  1, 62,  ..., 41, 51, 44],\n",
      "        [ 0,  0, 17,  ..., 47,  5,  1],\n",
      "        ...,\n",
      "        [45, 54, 57,  ..., 43,  1, 46],\n",
      "        [40, 59,  1,  ...,  1, 40, 57],\n",
      "        [54, 52, 44,  ..., 59, 47, 44]])\n",
      "tensor([[23, 12, 32,  ..., 57, 44,  1],\n",
      "        [ 1, 62, 48,  ..., 51, 44, 43],\n",
      "        [ 0, 17, 16,  ...,  5,  1, 41],\n",
      "        ...,\n",
      "        [54, 57,  1,  ...,  1, 46, 48],\n",
      "        [59,  1, 40,  ..., 40, 57, 44],\n",
      "        [52, 44,  1,  ..., 47, 44, 64]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderOnlyTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[43mvocab_size\u001b[49m, n_embd) \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(block_size, n_embd) \u001b[38;5;66;03m#Positional Embedding, depends on block size\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[DecoderBlock(n_embd, n_head\u001b[38;5;241m=\u001b[39mn_head) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def training_loop(model, dataLoader, optimizer):\n",
    "    train_loader = dataLoader.build_loader(\"train\")\n",
    "    valid_loader = dataLoader.build_loader(\"valid\")\n",
    "\n",
    "    # Training Phase\n",
    "    losses = []\n",
    "    valids = []\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        loss_i = 0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            logits, loss = model(x, y)\n",
    "            loss_i += loss.item()\n",
    "            # Backward Pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = loss_i / len(train_loader)\n",
    "        losses.append(avg_train_loss)\n",
    "        print(f'Epoch {i} training complete, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        valid_i = 0\n",
    "        with torch.no_grad():\n",
    "            for x_v, y_v in valid_loader:\n",
    "                x_v = x_v.to(device)\n",
    "                y_v = y_v.to(device)\n",
    "                logits, loss = model(x_v, y_v)\n",
    "                valid_i += loss.item()\n",
    "            avg_valid_loss = valid_i / len(valid_loader)\n",
    "            valids.append(avg_valid_loss)\n",
    "            print(f'Iteration {i}, Average validation loss: {avg_valid_loss}')\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        print(dataLoader.decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "    return losses, valids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Data('/kaggle/input/text-data/data/shakespeare.txt')\n",
    "dataLoader = Dataloader(data)\n",
    "model = DecoderOnlyTransformer(data)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "l, v = training_loop(model, dataLoader, optimizer)\n",
    "    \n",
    "torch.save(model.state_dict(), 'transformer_model.pth')\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dataLoader.decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
